% !TEX root = book.tex
\chapter{Linear algebra}
\label{linalg}
% use \chaptermark{} to alter or adjust the chapter heading in the running head
\section{Vector spaces}
Exactly as we did with fields or topological spaces, we will define vector
spaces as a tuple with some properties. \textbf{Vector space}\footnote{Sometimes vector spaces
are also called\textbf{linear spaces}} over a field $F$ is a tuple: $(F, V, +, \cdot)$, where the objects involved are:
\begin{itemize}
  \item $F$ is a field. It's elements are called \textbf{scalars}.
  \item $V$ is a set. It's elements are called \textbf{vectors}.
  \item $+$ is a function $V\times V\to V$. We always write $v+u$ instead of $+(v,u)$.
  \item $\cdot$ is a function $F\times V\to V$. We always write $f\cdot v$ instead of $\cdot(f,v)$.
\end{itemize}
They should have the following properties:
\begin{enumerate}
  \item $(u+v)+w=u+(v+w)$ for every $u,v,w\in V$ (associativity of addition)
  \item $u+v=v+u$ for every $u,v\in V$ (commutativity)
  \item There is $o\in V$ such that $v+o=v$ for all $v$ (neutral element of addition)
  \item For every $v\in V$ there is a $\tilde v\in V$ such that $v+\tilde v = o$ (additive inverse)
  \item For every $f,g\in F$ and $v\in V$ we have $(fg)\cdot V=f\cdot (g\cdot V)$ (so the multiplication agrees with that for scalars)
  \item As $F$ is a field, we have $1\in F$. For every $v\in V$ we want $1\cdot v=v$.
  \item For every $f\in F$ and $u, v \in V$ we have $f\cdot (u+v) = f\cdot u + f\cdot v$ (distributivity)
  \item For every $f, g\in F$ and $u\in V$ we have $(f+g)\cdot u=f\cdot u + g\cdot u$
\end{enumerate}

\begin{prob}
  We know that the set of vectors must contain at least one vector (neutral element). Construct a vector space that has \textit{exactly} one vector (so in some sense
  it is the smallest space).
\end{prob}

It's high time we started to abuse our notation making it less explicit, but more convenient. First of all we usually ommit
(as in the case of field) $\cdot$ for multiplication: $fv=f\cdot v$, for $f\in F, v\in V$. But that's not all!
\begin{prob}
  We want to modify our notation in the following way:
  \begin{enumerate}
    \item Prove that $o$ is unique element with the property $v+o=v$ for $v\in V$
    \item Prove that $0\cdot v=o$ for every $v$. Hint: remember that $1+0=1$. This suggests to write 0 for $o$ (so 0 since now technically has two different meanings,
      practically we will never have any problems with that)
    \item Let $v\in V$ and $\tilde v\in V$ be such an element that $v+\tilde v=0$. Prove that $\tilde v$ is unique (so if $v'+v=0$, then $\tilde v=v'$)
    \item Prove that the $\tilde v$ is exactly $(-1)v$. It suggests to write $-v$ for additive inverse, and we will do it since now.
  \end{enumerate}
\end{prob}

Moreover, if we specify the field od scalars and operations, we will say $V$ for the vector space, without invoking the all tuple elements (as in the case with topological spaces or fields- we write a single $\mathbb R$ and everyone knows what field operations we allow and what topology is assumed).

In most cases we will be interested in non-patological vector spaces, namely ,,big enough" in some sense. Why?

\begin{prob}
  The \textbf{characteristic} of a field $F$ is the smallest natual number $n$ such that $1+1+\dots+1=0$, where we have $n$ ones on the left hand side. If there is no such number
  we say that characteristic is 0.
  \begin{enumerate}
    \item Prove that $\mathbb R$ has characteristic 0.
    \item Let $(F, V)$ be a vector space with at least two elements. Prove that $v=-v$ for every $v\in V$ if and only if the scalar field has characteristic 2.
    \item Prove that if $F$ has characteristic different from 2, then we have $v=-v$ iff $v=0$.
  \end{enumerate}
\end{prob}

\begin{prob}
  An important example of a vector space over a field $F$ is $F^n$, where addition and scalar multiplication are defined pointwise:
  $f\cdot (a_1, a_2, \dots, a_n) = (fa_1, fa_2, \dots, fa_n),~(a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n)=(a_1+b_1, a_2+b_2, \dots, a_n+b_n)$. Prove that it is indeed a vector space.
\end{prob}

\subsection{Bases of vectos spaces}
In topology we introduced a basis as a family of open sets from each every open set could be constructed in some natural way. This useful concept occurs also in vector spaces -
as you can see, $F^n$ has an interesting property: each element $(a_1, a_2, \dots, a_n)\in F^n$ can be written in a form $a_1e_1+a_2e_2+\dots+a_ne_n$, where $e_k$ has 1 at $k$-th place
and 0s in the other. Moreover, if $\lambda_1, \lambda_2, \dots, \lambda_n\in F$, then the only possibility for the equation $\lambda_1e_1+\lambda_2e_2+\dots+\lambda_ne_n=0$ to hold
is $\lambda_1=\lambda_2=\dots=\lambda_n=0$. This suggests the following definitions: let $U\subseteq V$. A set $\Span U$ is defined as:
$$ \Span U = \{\lambda_1u_1+\lambda_2u_2+\dots+\lambda_nu_n : \lambda_1, \dots, \lambda_n\in F, u_1, \dots, u_n\in U\}.$$
Alternatively, we can say that $\Span U$ is a subset of $V$ such that each $v\in \Span U$ can be wrtitten as a finite \textbf{linear combination} of elements from $U$. It is important
to \textit{disallow} infinite combinations - the concept of an infinite sum is essentialy topological and we have \textit{not} assumed any topology on our space yet! Therefore we cannot define convergence and infinite sums.

\begin{prob}
  Consider infinite real sequences with addition and multiplication by a real number defined pointwise: $c=a+b$ iff $c_n=a_n+b_n$ for all $n$ and $b=ra, r\in \mathbb R$ iff $b_n=ra_n$ for all $n$.
  \begin{enumerate}
    \item Prove that this is a vector space, let's call it $\mathbb R^{\mathbb N}$.
    \item Prove that set $B=\{e_k : k\in \mathbb N\},~e_k$ has 1 at $k$-th place and 0 at all the others, does \textit{not} span $\mathbb R^{\mathbb N}$.
    \item Let $\hat{\mathbb R}^{\mathbb N}\subseteq \mathbb R^{\mathbb N}$ contain all the sequences that have a \textit{finite} number of non-zero elements. Prove that this is a
      vector space and that it is spanned by $B$ defined above.
  \end{enumerate}
\end{prob}

Moreover we will say that a \textit{finite} set of vectors $\{v_1, v_2, \dots, v_n\}$ is \textbf{linearly independent} if the only solution for
$$\lambda_1v_1+\lambda_2v_2+\dots+\lambda_nv_n=0$$
is $\lambda_1=\lambda_2=\dots=\lambda_n=0$. If vectors are \textit{not} linearly independent, we say that they are \textbf{linearly dependent}.

\begin{prob}
  Assume that $v_1, v_2, \dots, v_n$ are \textit{not} linearly dependent. Prove that there is $v_i$ such that $v_i$ is a linear combination of other vectors:
  $v_i \in \Span \{v_1, \dots, v_{i-1}, v_{i+1}, v_n\}$.
\end{prob}

We say that a vector space $V$ has \textbf{finite dimension} (or is \textit{finite dimensional}) if there is a \textit{finite} $U\subseteq V$ such that $V=\Span U$.
\begin{prob}
  You can prove that every finite dimensional vector space has a basis in two steps:
    \begin{enumerate}
      \item Assume that you have a set $\{e_1, e_2,\dots, e_n\}$ that spans $V$. Prove that if $e_1\in \Span \{e_2,\dots, e_n\}$, then $V=\Span \{e_2,\dots, e_n\}$.
      \item Using the reduction step given above, show an algorithm finding a basis from a finite spanning set.
      \item Prove that a vector space is finite dimensional iff it has a finite basis.
    \end{enumerate}
\end{prob}

\begin{prob}
  Prove that $F^n$ is finite dimensional. Hint: just find a basis.
\end{prob}

\begin{prob}
  Prove that $\hat{\mathbb R}^{\mathbb N}$ has a basis.
\end{prob}

Apparently the proof that \textit{every} vector space is equivalent to the Axiom of Choice!
% Proof using Zorn's lemma that _every_ vector space has a basis.

\begin{prob}
  Here you will prove that all the bases of a \textit{finite dimensional} vector space have the same number of elements. Let $v_1, v_2,\dots, v_n$ be a basis of a vector space $V$ and $w_1,w_2,\dots,w_m\in V$, where $m > n$.
  \begin{enumerate}
    \item \textbf{(Steinitz exchange lemma)} Prove that if $w_1\neq 0$, then $v_1\in \Span \{w_1, v_2, v_3, \dots,v_n\}$.
    \item Prove that if $w_k\neq 0$ for $k\in\{1, 2, \dots, n\},$ then $w_{n+1}\in V=\Span\{w_1, w_2, \dots, w_n\}$
    \item Prove that $w_1, w_2, \dots, w_m$ \textit{cannot} be linearly independent.
    \item Prove that each basis of $V$ has the same number of elements. This number is called \textbf{the dimension of $V$} and written as $\dim V$.
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $V$ be a finite dimensional vector space of dimension $n$. Prove that every linearly independent set of $n$ vectors spans $V$ and every set with $n$ elements spanning $V$
  is a basis.
\end{prob}

\begin{prob}
  Here you will prove that every linearly independent set of vectors can be extended to a basis of a finite dimensional vector space. Let $V$ be a finite dimensional vector space
  of dimension $n$.
  \begin{enumerate}
    \item Let $S=\{v_1,\, v_2,\dots,\, v_k\}\subseteq V$ be linearly independent. Prove that if $u\in V$, but $u\notin \Span S$, then $\{u\}\cup S$ is linearly independent.
    \item Prove that there are $u_1, u_2, \dots,u_{n-k}$ such that $v_1, v_2, \dots, v_k, u_1, u_2,\dots, u_{n-k}$ is a basis of $V$.
  \end{enumerate}
\end{prob}

\begin{prob}
  Assume that you have a basis $e_1, e_2,\dots,e_n$ of a finite dimensional vector space $V$ over a field $\mathbb F$. Therefore every vector $v$ can be written as a sum
  $v=v_1e_1+v_2e_2+\dots+v_ne_n$ for some $v_i\in \mathbb F$. Prove that these numbers are unique, that is if $v=v_1e_1+v_2e_2+\dots+v_ne_n=v'_1e_1+v'_2e_2+\dots+v'_ne_n$, then
  $v_i=v'_i$ for all $i$. Hint: $0=v-v$ and $e_i$ are linearly independent.
\end{prob}

\subsection{Subspaces, direct sum and quotient spaces}
As in the case of topological spaces, there are many ways of constructing \textit{new} vector spaces from old. In topology we could construct
new spaces taking a subset of a known topological space, take disjoint unions (sums) of topological spaces, divide topological spaces by some relations
and take product of them. In this subsection we cover first three constructions - fourth one gives a raise to the concept of tensors and multilinear algebra and we will cover
it in great detail later.
Consider a vector space $V$ over field $F$ and a subset $U\subset V$ such that $0\in U$ and for all $v, u\in U, f\in F$, we have $fv+u\in U$. You can check that it is indeed a vector space:
\begin{prob}
  Prove that $fv+u\in U$ for all $v,\, u\in U, f\in F$ is equivalent to: for every $v,\, u\in U$, we have $v+u\in U$ and for every $v\in U, f\in F$ we have $fv\in U$.
\end{prob}

Such a $U$ we call a \textbf{vector subspace} of $V$.

\begin{prob}
  Let $V$ be a finite dimensional vector space and $U\subseteq V$ be a vector subspace. Prove that $U$ is finite dimensional and $\dim U\le \dim V.$
\end{prob}

\begin{prob}
  Let $V$ be a finite dimensional vector space and $U\subseteq V$ be a vector subspace. Prove that $\dim U=\dim V$ iff $U=V$.
\end{prob}

There is also a method of constructing direct sums: assume that you have two vector spaces $V, W$ over \textit{the same field} $F$. We define their direct sum as:
$$V\oplus W=V\times W = \{(v,w) : v\in V,\, w\in W\}$$
with addition and multiplication defined entrywise:
$$a(v,w)+(v', w')=(av+v', aw+w').$$

We often identify $v\in V$ with $(v,0)\in V\oplus W$ and $w\in W$ with $(0,w)\in V\oplus W$. Then $av+bu, a,b\in F, v\in V, u\in U$ should be understood as $(av, bu)\in V\oplus U$.

\begin{prob}
  Prove that each $w\in V\oplus U$ has a \textit{unique} decomposition: $w=v+u,\, v\in V,\,u\in U$. That is if $w=v+u=v'+u'$, then $v=v'$ and $u=u'$ for $v,v'\in V,\, u,u'\in U$.
\end{prob}

\begin{prob}
  Let $U$ and $V$ be finite dimensional vector spaces. Prove that $\dim U\oplus V = \dim U + \dim V.$
\end{prob}

\begin{prob}
  Let $V_1, \,V_2,\, V_n$ be finite dimensional vector spaces. Prove that $$\dim V_1\oplus V_2\oplus\dots\oplus V_n=\dim V_1 + \dim V_2 + \dots + \dim V_n.$$
\end{prob}

Our general definition has a very nice interpretation when we go to subspaces -
now assume that you have two subspaces of $V$: $U,W\subseteq V$ such that $U\cap W=0$. Their \textbf{direct product} is a set:
$$U\oplus W =\{u+v : u\in U,\, v\in V\}$$

\begin{prob}
  Prove that the direct product of two vector subspaces is a special case of the general definition if we identify $U\ni u\leftrightarrow(u,0)\in U\times V$, $V\ni v\leftrightarrow (0,v)\in U\times V$ employed.
\end{prob}

\begin{prob}
  Prove directly that direct product of two vector subspaces of $V$ is a vector subspace of $V$. Hint: check if 0 is inside and use the handy, one-line criterion.
\end{prob}

\begin{prob}
  Let $U$ be a subspace of $V$ and $V$ be a subspace of $W$. Prove that $U$ is subspace of $W$.
\end{prob}

\begin{prob}
  Let $V=\mathbb R^2$ and $U=\{(0, r) : r\in \mathbb R\},~W=\{(r, 0) : r\in \mathbb R\}$. Prove that:
  \begin{enumerate}
    \item $V=U\oplus W$.
    \item Prove that $U\cup V$ is \textit{not} a vector space.
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $U,W\subseteq V$ be two vector subspaces of a finite vector space $V$. Prove that:
  \begin{enumerate}
    \item $U\cap W$ is a subspace of $U$, $W$ and $V$.
    \item $U+W:=\{u+w : u\in U, w\in W\}$ is a vector subspace\footnote{if $U\cap W=\emptyset$ we have just $U+W=U\oplus W$} of $V$
    \item Take a basis $B_i$ of $U\cap W$ and extend it using some vectors $B_U\subseteq U$ such that $B_i\cup B_U$ is a basis of $U$.
      Repeat this procedure of $W$ defining $B_W$ and prove that $V=\Span B_i\cup B_U\cup B_W.$
    \item Prove that $B_i\cup B_U\cup B_V$ is linearly independent. Hint: write the condition of linear independence. Express the linear combination of the elements of $B_U$ as a linear combination of $B_i$ and $B_V$. Why is this linear combination in $U\cap V$? What you can conclude from the fact that $B_i\cup B_U$ is a basis?
    \item Prove that $\dim U+V=\dim U+\dim V - \dim U\cap V$.
  \end{enumerate}
\end{prob}

\subsection{Quotient spaces}
Let us introduce a relation:
\begin{prob}
  Consider vector space $V$ and it's subspace $U$. We introduce a relation on $V$: $v\approx u$ iff $v-u\in V$. Prove that $\approx$ is an equivalence relation.
\end{prob}
We have an equivalence relation, so it splits $V$ into equivalence classes $V/\approx$.

\begin{prob}
  Prove that addition and scalar multiplication on $V/\approx$ are well-defined (independent on the class representative), that is:
  \begin{enumerate}
    \item if $v\approx v'$ and $u\approx u'$, then $v+u\approx v'+u'$
    \item if $\alpha$ is a scalar and $v\approx v'$ are vectors in $V$, then $\alpha v\approx \alpha v'$.
  \end{enumerate}
\end{prob}

\begin{prob}
  Prove that under relation $\approx$, $U$ is identified with 0.
\end{prob}

We have vector addition and scalar multiplication, we have a neutral element - we have a new vector space! This vector space is called \textbf{quotient space} and usually written as
$V/U$.

\begin{prob}
  Let $U\subseteq V$ be finite-dimensional vector spaces. Prove that $\dim V/U=\dim V - \dim U$. Hint: guess what is the basis of $V/U$ starting with basis of $U$ and completing
  it to the basis ov $V$.
\end{prob}

\section{Linear maps}
As continous functions are in some kind, natural mappings between topological spaces, we can define such natural mappings between vector spaces. Let $V$ and $W$ be vector spaces over
the same field $\mathbb F$. We say that a function $L:V\to W$ is \textbf{linear} iff $L(\alpha v + \beta u)=\alpha L(v) + \beta L(u)$ for every $v,u\in V,\, \alpha,\beta\in \mathbb F$.

\begin{prob}
  Let $L:V\to W$ be a function between vector spaces over field $\mathbb F$. Prove that the following sentences are equivalent:
  \begin{enumerate}
    \item $L$ is linear
    \item for every $u,v\in V$ and $\alpha\in \mathbb F$, we have $L(\alpha u+v)=\alpha L(u)+L(v)$
    \item for every $v,u\in V$ we have $L(v+u)=L(v)+L(u)$ and for every $v\in V, \alpha\in \mathbb F$ we have $L(\alpha v) = \alpha L(v)$
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $U$ be a vector subspace of $V$. Prove that \textbf{the inclusion map} $\iota : U\to V$ given as $\iota(u)=u$ is linear.
\end{prob}

\begin{prob}
  Let $U$ be a vector subspace of $V$. Prove that \textbf{the quotient map} $q: V\to V/U$ given as $q(v)=[v]$ is linear.
\end{prob}

\subsection{Kernel and cokernel}
\begin{prob}
  Let $L:V\to W$ be a linear map between vector spaces.
  \begin{enumerate}
    \item Prove that $L(0_V)=0_W$, where $0_V$ is the neutral element in $V$ and $0_W$ is the neutral element in $W$.
    \item Prove that the \textbf{kernel of} $L$ defined as: $\Ker L = \{v\in V : L(v) = 0_W\}$ is a vector subspace of $V$.
    \item Prove that the image of $L$ is a vector subspace of $W$. The dimension of $\Image L$ is called \textbf{the rank} of $L$: $\Rank L = \dim \Image L$.
    \item Let $V=\Span S$. Prove that $\Image L=\Span L(S)$. Here $L(S)$ has meaning $\{L(s) : s\in S\}$.
    \item Prove that if $V$ is finite dimensional, then $\Image L$ is also finite dimensional.
  \end{enumerate}
\end{prob}

Using kernel we can say whether a linear function is injective:
\begin{prob}
  Prove that $\varphi : V\to U$ is injective iff $\Ker \varphi = \{0\}$.
\end{prob}

There is a similar concept, checking the surjectivity. We say that the cokernel of a linear map $L: V\to U$ is a \textit{quotient} vector space: $U/\Image L.$

\begin{prob}
  Prove that $\varphi : V\to U$ is surjective iff $\Coker \varphi$ has exactly one element (is a trivial vector space).
\end{prob}

\subsection{Rank-nullity theorem}
You can prove the rank-nullity theorem:
\begin{prob}
  Prove the \textbf{rank-nullity theorem} - if $V$ is a finite dimensional vector space and $L:V\to W$ is linear, then $\dim \Ker L + \Rank L=\dim V$.
\end{prob}

\begin{prob}
  You can do a beuatiful and simple proof of $\dim U+V = \dim U + \dim V - \dim U\cap V$, where $U$ and $V$ are finite-dimensional subspaces of some greater space $S$.
  Consider a map $L: U\times V\to S$ defined as $L(u,w)=u-w$. Prove that it is a linear map (we give $U\times V$ a linear space structure using entry-wise operations, as in
  $U\oplus V$). What is it's range and kernel?
\end{prob}

\begin{prob}
  Let $\varphi : V\to U$ be a map between finite-dimensional vector spaces. Prove that the \textbf{index} of $\varphi$ defined as $\text{index\,} \varphi=\dim \Ker \varphi - \dim \Coker \varphi$ is equal to:
   $\text{index\,} \varphi=\dim V - \dim U$.\footnote{This is a very important result - index, defined with the help of a chosen function is doesn't in fact depend on this function! A beautiful generalisation of this result, is called Atiyah-Singer index theorem.}
\end{prob}

In the category of topological spaces, homeomorphic spaces have the same topological properties (as connectedness or Haussdorf property). We say that a map $L:W\to W$ is a
\textbf{isomorphism\footnote{from the Ancient Greek \textit{isos} - equal and \textit{morphe} - shape} of vector spaces} iff is bijective and both $L$ and $L^{-1}$ are linear and
we can say that abstract\footnote{Later we will define additional structures on vector spaces for which just arbitrary isomorphisms are not sufficient} vector spaces are identical if they are isomorphic.

\begin{prob}
  Prove that $V$ and $\{0\}\times V$ are isomorphic. (Do you remember the direct sum of two subspaces? In fact we used there this isomorphism).
\end{prob}

You can also classify all finite-dimensional vector spaces over the same field, up to isomorphism:

\begin{prob}
  Prove that two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension. Hint: right implication - rank-nullity theorem, left - write down bases.
\end{prob}

As we remember $\F^n$ is an $n$-dimensional vector space over $\F$. Therefore many people learn how to work only on them, as all the vector space properties can be
just transferred from $\F^n$. We will not do it, as the isomorphism is usually not-unique and does not preserve additional structures, very important in more advanced geometry.
Such chosen isomorphisms will later give a raise to objects called metric tensors and symplectic forms.

\subsection{Exact sequences}
In algebraic topology topological spaces are investigated by assigning to them algebraic objects, like vector spaces or groups. One of frequently-occuring concept are exact sequences.
Consider a sequence of vector spaces\footnote{Or, more generally, Abelian groups as we will need only to use such properties as kernel, image and quotient spaces.} $V_i,~i\in \{1,2,\dots\}$ and linear maps between them $\varphi_i : V_i\to V_{i+1}$. It is often written as:
$$\dots\stackrel{\varphi_{i-1}}{\longrightarrow} V_i\stackrel{\varphi_i}\longrightarrow V_{i+1} \stackrel{\varphi_{i+1}}\longrightarrow \dots,$$
and map $\varphi_i$ is referenced as $V_i\to V_{i+1}$. We say that the sequence is \textbf{exact} if $\Image \varphi_{i}=\Ker \varphi_{i+1}$ for all $i$.

\begin{prob}
  Prove that if sequence of vector spaces $V_i$ and maps $\varphi_i : V_i\to V_{i+1}$, is exact, then $\varphi_i\circ \varphi_{i+1}=\textbf{0}$, where \textbf{0} is a null \footnote{Later we will refer to this map just as 0 - now this symbol has at least three meanings! It can be an additive neutral element of a field, a neutral element in a vector space or a linear map!} map $\textbf{0}: V_i\to V_{i+2}$ defined as $\textbf{0}(v)=0$.
\end{prob}

Consider a $0$-dimensional vector space $\{0\}$, which is usually abbreviated to just 0\footnote{Similar notational discrepancy was in the set theory - we wanted to write
$f^{-1}(a)$ for $f^{-1}(\{a\})$.

}. An exact sequence (Remember! Here 0 means $\{0\}$):
$$0\stackrel{\varphi_1}{\longrightarrow} V_2\stackrel{\varphi_2}{\longrightarrow} V_3\stackrel{\varphi_3}{\longrightarrow} V_4\stackrel{\varphi_4}{\longrightarrow} 0$$
is called a \textbf{short exact sequence}. Longer exact sequences are called, obviously, \textbf{long exact sequences}.

\begin{prob}
  Prove the following:
  \begin{enumerate}
    \item sequence $V\stackrel{\varphi}{\longrightarrow}U\longrightarrow 0$ is exact iff $\varphi$ is surjective. Hint: what is kernel of the map $U\to 0$?
    \item sequence $0\longrightarrow V\stackrel{\varphi}{\longrightarrow}U$ is exact iff $\varphi$ is injective. Hint: do you remember how injectivity is related to some kernel?
    \item sequence $0\longrightarrow V\stackrel{\varphi}{\longrightarrow}U\longrightarrow 0$ is exact iff $V$ and $U$ are isomorphic.
    \item short sequence
      $0\longrightarrow V_2\stackrel{\varphi_2}{\longrightarrow} V_3\stackrel{\varphi_3}{\longrightarrow} V_4\longrightarrow 0$
      is exact iff $\varphi_2$ is injective and $\varphi_3$ is surjective.
  \end{enumerate}
\end{prob}

\begin{prob}
  Use the rank-nullity theorem and prove that if $0\to V_0\to V_1\to\dots\to V_{n-1}\to V_n\to 0$ is exact, then
  $$0=\sum_{i=0}^n (-1)^i \dim V_i.$$
\end{prob}

\begin{prob}
  Consider a linear map $L : V\to U$. Prove that the sequence:
  $$0\longrightarrow \Ker L\stackrel{\kappa}{\longrightarrow} V\stackrel{L}{\longrightarrow} U\longrightarrow \Coker L \longrightarrow 0$$
  is exact, where $\kappa : \ker L \to V$ is inclusion map: $\kappa :\Ker L\ni l\to l\in V$.
\end{prob}

\begin{prob}
  Let $L : V\to U$ be a map between finite-dimensional vector spaces. Prove once again that the $\text{index\,} L:=\dim \Ker L - \dim \Coker L =\dim V - \dim U$.
\end{prob}

\section{Dual spaces}
Consider a vector space $V$ over a field $\F$ and the set of all \textit{linear} functions from $V$ to $\F$. This set is called
\textbf{the dual space} $V^*$. Apparently, this set is a vector space!
\begin{prob}
  Prove that $V^*$ is a vector space by:
  \begin{enumerate}
    \item Finding the neutral element. Hint: function that always yields 0 is linear.
    \item Proving that addition and scalar multiplication can be defined, so if $\mu,\,\omega \in V^*$ and $a\in \F$, then function $a\mu +\omega$, defined as:
     $(a\mu +\omega)(v) = a\cdot \mu(v) +\omega(v)$ for all $v\in V$, is linear.
  \end{enumerate}
\end{prob}

Sometimes we write $\BL \omega, v\BR := \omega(v)\in \F$ for $v\in V$ and $\omega \in V^*$.

\begin{prob}
  Let $V^{**}=(V^*)^*$ be the dual space of the dual space to $V$. We will try to prove that, in some sense, $V$ is a subset of it. Prove that:
  \begin{enumerate}
    \item For each $v\in V$ there is a $\tilde v\in V^{**}$ such that for every $\omega \in V^*$ such that $\BL \tilde v, \omega\BR=\BL \omega, v\BR$
    \item Prove that the map $v\mapsto \tilde v$ is a monomorphism (an injective linear map).
  \end{enumerate}
\end{prob}

Therefore we can think about $V$ as a set of \textit{some} linear functions on $V^*$ and write $v(\omega)$ as well as $\omega(v)$. The word \textit{some} is necessary -
we have proven that it is a monomorphism, but we don't know whether it is an isomorphism\footnote{Unfortunately for vector spaces $V$ of \textit{infinite} dimension, $V^*$ is greater than $V$ and consequently $V^{**}$ is greater than both $V^*$ and $V$.}! Fortunately, there is a nice theorem for vector spaces of finite dimension:

\begin{prob}
  Let $V$ be a finite dimensional space with basis $e_1,e_2,\dots,e_n$. Prove that $\mu^1, \mu^2,\dots,\mu^n\in V^*$ defined as
    $$\mu^i(e_j)=\begin{cases}1 \text{ if } i=j\\ 0 \text{ otherwise}\end{cases},$$
  is a basis of $V^*$.
\end{prob}

As we know that in this case $V$ and $V^*$ have the same dimension, we can find an isomorphism between them - so they can be identified.
Unfortunately, for our future needs, just an isomorphism is not enough. We will be interested mostly in \textbf{natural} or \textbf{canonical} isomorphisms, that
is isomorphisms that don't need a basis to be defined.
