% !TEX root = book.tex
\chapter{Linear algebra}
\label{linalg}
% use \chaptermark{} to alter or adjust the chapter heading in the running head
\section{Vector spaces}
Exactly as we did with fields or topological spaces, we will define vector
spaces as a tuple with some properties. \textbf{Vector space}\footnote{Sometimes vector spaces
are also called\textbf{linear spaces}} over a field $F$ is a tuple: $(F, V, +, \cdot)$, where the objects involved are:
\begin{itemize}
  \item $F$ is a field. It's elements are called \textbf{scalars}.
  \item $V$ is a set. It's elements are called \textbf{vectors}.
  \item $+$ is a function $V\times V\to V$. We always write $v+u$ instead of $+(v,u)$.
  \item $\cdot$ is a function $F\times V\to V$. We always write $f\cdot v$ instead of $\cdot(f,v)$.
\end{itemize}
They should have the following properties:
\begin{enumerate}
  \item $(u+v)+w=u+(v+w)$ for every $u,v,w\in V$ (associativity of addition)
  \item $u+v=v+u$ for every $u,v\in V$ (commutativity)
  \item There is $o\in V$ such that $v+o=v$ for all $v$ (neutral element of addition)
  \item For every $v\in V$ there is a $\tilde v\in V$ such that $v+\tilde v = o$ (additive inverse)
  \item For every $f,g\in F$ and $v\in V$ we have $(fg)\cdot V=f\cdot (g\cdot V)$ (so the multiplication agrees with that for scalars)
  \item As $F$ is a field, we have $1\in F$. For every $v\in V$ we want $1\cdot v=v$.
  \item For every $f\in F$ and $u, v \in V$ we have $f\cdot (u+v) = f\cdot u + f\cdot v$ (distributivity)
  \item For every $f, g\in F$ and $u\in V$ we have $(f+g)\cdot u=f\cdot u + g\cdot u$
\end{enumerate}

\begin{prob}
  We know that the set of vectors must contain at least one vector (neutral element). Construct a vector space that has \textit{exactly} one vector (so in some sense
  it is the smallest space).
\end{prob}

It's high time we started to abuse our notation making it less explicit, but more convenient. First of all we usually ommit
(as in the case of field) $\cdot$ for multiplication: $fv=f\cdot v$, for $f\in F, v\in V$. But that's not all!
\begin{prob}
  We want to modify our notation in the following way:
  \begin{enumerate}
    \item Prove that $o$ is unique element with the property $v+o=v$ for $v\in V$
    \item Prove that $0\cdot v=o$ for every $v$. Hint: remember that $1+0=1$. This suggests to write 0 for $o$ (so 0 since now technically has two different meanings,
      practically we will never have any problems with that)
    \item Let $v\in V$ and $\tilde v\in V$ be such an element that $v+\tilde v=0$. Prove that $\tilde v$ is unique (so if $v'+v=0$, then $\tilde v=v'$)
    \item Prove that the $\tilde v$ is exactly $(-1)v$. It suggests to write $-v$ for additive inverse, and we will do it since now.
  \end{enumerate}
\end{prob}

Moreover, if we specify the field od scalars and operations, we will say $V$ for the vector space, without invoking the all tuple elements (as in the case with topological spaces or fields- we write a single $\mathbb R$ and everyone knows what field operations we allow and what topology is assumed).

In most cases we will be interested in non-patological vector spaces, namely ,,big enough" in some sense. Why?

\begin{prob}
  The \textbf{characteristic} of a field $F$ is the smallest natual number $n$ such that $1+1+\dots+1=0$, where we have $n$ ones on the left hand side. If there is no such number
  we say that characteristic is 0.
  \begin{enumerate}
    \item Prove that $\mathbb R$ has characteristic 0.
    \item Let $(F, V)$ be a vector space with at least two elements. Prove that $v=-v$ for every $v\in V$ if and only if the scalar field has characteristic 2.
    \item Prove that if $F$ has characteristic different from 2, then we have $v=-v$ iff $v=0$.
  \end{enumerate}
\end{prob}

\begin{prob}
  An important example of a vector space over a field $F$ is $F^n$, where addition and scalar multiplication are defined pointwise:
  $f\cdot (a_1, a_2, \dots, a_n) = (fa_1, fa_2, \dots, fa_n),~(a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n)=(a_1+b_1, a_2+b_2, \dots, a_n+b_n)$. Prove that it is indeed a vector space.
\end{prob}

\subsection{Bases of vectos spaces}
In topology we introduced a basis as a family of open sets from each every open set could be constructed in some natural way. This useful concept occurs also in vector spaces -
as you can see, $F^n$ has an interesting property: each element $(a_1, a_2, \dots, a_n)\in F^n$ can be written in a form $a_1e_1+a_2e_2+\dots+a_ne_n$, where $e_k$ has 1 at $k$-th place
and 0s in the other. Moreover, if $\lambda_1, \lambda_2, \dots, \lambda_n\in F$, then the only possibility for the equation $\lambda_1e_1+\lambda_2e_2+\dots+\lambda_ne_n=0$ to hold
is $\lambda_1=\lambda_2=\dots=\lambda_n=0$. This suggests the following definitions: let $U\subseteq V$. A set $\Span U$ is defined as:
$$ \Span U = \{\lambda_1u_1+\lambda_2u_2+\dots+\lambda_nu_n : \lambda_1, \dots, \lambda_n\in F, u_1, \dots, u_n\in U\}.$$
Alternatively, we can say that $\Span U$ is a subset of $V$ such that each $v\in \Span U$ can be wrtitten as a finite \textbf{linear combination} of elements from $U$. It is important
to \textit{disallow} infinite combinations - the concept of an infinite sum is essentialy topological and we have \textit{not} assumed any topology on our space yet! Therefore we cannot define convergence and infinite sums.

\begin{prob}
  Consider infinite real sequences with addition and multiplication by a real number defined pointwise: $c=a+b$ iff $c_n=a_n+b_n$ for all $n$ and $b=ra, r\in \mathbb R$ iff $b_n=ra_n$ for all $n$.
  \begin{enumerate}
    \item Prove that this is a vector space, let's call it $\mathbb R^{\mathbb N}$.
    \item Prove that set $B=\{e_k : k\in \mathbb N\},~e_k$ has 1 at $k$-th place and 0 at all the others, does \textit{not} span $\mathbb R^{\mathbb N}$.
    \item Let $\hat{\mathbb R}^{\mathbb N}\subseteq \mathbb R^{\mathbb N}$ contain all the sequences that have a \textit{finite} number of non-zero elements. Prove that this is a
      vector space and that it is spanned by $B$ defined above.
  \end{enumerate}
\end{prob}

\begin{definition}
  % Linear independence of a set of vectors
  Let $V$ be a vector space. We say that set $U\subseteq V$ is \textbf{linearly independent} if for every finite subset of $U$: $\{v_1, v_2, \dots, v_n\}$,
  the only solution of the equation
  $$\lambda_1v_1+\lambda_2v_2+\dots+\lambda_nv_n=0$$
  is trivial: $\lambda_1=\lambda_2=\dots=\lambda_n=0$.
  If a set of vectors is \textit{not} linearly independent, we say that it is \textbf{linearly dependent}.
  Sometimes we abuse the terminology and say that \textit{vectors} $v_1, v_2, \dots, v_n$ are linearly independent - it means that the set $\{v_1,v_2\dots, v_n\}$ is linearly
  independent - linear independence is a property of a set of vectors, not a property of a single vector!
\end{definition}

\begin{exercise}
  % Linear independence of a finite set of vectors
  Let $V$ be a vector space and consider a finite set $\{v_1, v_2, \dots, v_n\}\subseteq V$. Prove that it is linearly independent iff the only solution to the equation:
  $$\lambda_1v_1+\lambda_2v_2+\dots+\lambda_nv_n=0$$
  is $\lambda_1=\lambda_2=\dots=\lambda_n=0$.
\end{exercise}

\begin{exercise}
  % Linear dependence of n > 1 vectors <=> at least one vector is a linear combination of others
  Consider a finite set $U=\{v_1, v_2, \dots, v_n\}$ with at least two vectors. Prove that the following two statements are equivalent:
  \begin{itemize}
    \item $U$ is linearly dependent
    \item there is $v_i\in U$ such can be written as linear combination of other vectors: $v_i \in \Span \{v_1, \dots, v_{i-1}, v_{i+1}, v_n\}$
  \end{itemize}
\end{exercise}

\begin{definition}
  % Finite dimensional space
  We say that a vector space $V$ has \textbf{finite dimension} (or is \textit{finite dimensional}) if there is a \textit{finite} $U\subseteq V$ such that $V=\Span U$.
\end{definition}

\begin{exercise}
  % Every finite dimensional space has a basis
  You can prove that every finite dimensional vector space has a basis in two steps:
    \begin{enumerate}
      \item Assume that you have a set $\{e_1, e_2,\dots, e_n\}$ that spans $V$. Prove that if $e_1\in \Span \{e_2,\dots, e_n\}$, then $V=\Span \{e_2,\dots, e_n\}$.
      \item Using the reduction step given above, show an algorithm finding a basis from a finite spanning set.
      \item Prove that a vector space is finite dimensional iff it has a finite basis.
    \end{enumerate}
\end{exercise}

\begin{prob}
  % F^n is finite dimensional
  Prove that $\F^n$ is finite dimensional. Hint: just find a basis.
\end{prob}

\begin{prob}
  Prove that $\hat{\mathbb R}^{\mathbb N}$ has a basis.
\end{prob}

Apparently the proof that \textit{every} vector space is equivalent\footnote{Proof that AC is implied by statement "every vector space has a basis"
was given by Andreas Blass in 1984! It can be found here:
\texttt{http://www.math.lsa.umich.edu/~ablass/bases-AC.pdf}} to the Axiom of Choice!

\begin{exercise}
  % Proof using Zorn's lemma that every vector space has a basis.
  You can see how to prove the basis existence with the help of Zorn's lemma. Let $V$ be a vector space.
  \begin{enumerate}
    \item Let $\mathcal A = \{U\subseteq V : U\text{ is linearly independent}\}$. Prove that $\mathcal A$ is not empty.
    \item Prove that relation on $A$ given by $A\preceq B \Leftrightarrow A\subseteq B$ is a partial order.
    \item Consider any chain $\mathcal C\subseteq \mathcal A$. Define $C=\bigcup\mathcal C$. We want to prove that $C$ is linearly independent.
    \item Assume that $C$ is linearly dependent, so $0=\lambda_1v_1+\dots\lambda_nv_n$ for some $v_i\in C$. If $v_i\in C_i\in \mathcal C$, what
      can you conclude about $C_1\cup C_2\cup\dots\cup C_n$?
    \item From Zorn's lemma we know that there is a \textit{maximal} element $A$ in $\mathcal A$. What if $A$ does not span $V$? Hint: add an element
      that is not in the span and think about linear independence of new set. $A$ is maximal, isn't it?
  \end{enumerate}
\end{exercise}

\begin{prob}
  Here you will prove that all the bases of a \textit{finite dimensional} vector space have the same number of elements. Let $v_1, v_2,\dots, v_n$ be a basis of a vector space $V$ and $w_1,w_2,\dots,w_m\in V$, where $m > n$.
  \begin{enumerate}
    \item \textbf{(Steinitz exchange lemma)} Prove that if $w_1\neq 0$, then $v_1\in \Span \{w_1, v_2, v_3, \dots,v_n\}$.
    \item Prove that if $w_k\neq 0$ for $k\in\{1, 2, \dots, n\},$ then $w_{n+1}\in V=\Span\{w_1, w_2, \dots, w_n\}$
    \item Prove that $w_1, w_2, \dots, w_m$ \textit{cannot} be linearly independent.
    \item Prove that each basis of $V$ has the same number of elements. This number is called \textbf{the dimension of $V$} and written as $\dim V$.
  \end{enumerate}
\end{prob}

\begin{exercise}
  Let $V$ be a finite dimensional vector space of dimension $n$. Prove that:
  \begin{enumerate}
  \item every linearly independent set of $n$ vectors spans $V$ (so must span $V$)
  \item every set with $n$ elements spanning $V$ is a basis (so must be linearly independent)
  \end{enumerate}
\end{exercise}

\begin{prob}
  Here you will prove that every linearly independent set of vectors can be extended to a basis of a finite dimensional vector space. Let $V$ be a finite dimensional vector space
  of dimension $n$.
  \begin{enumerate}
    \item Let $S=\{v_1,\, v_2,\dots,\, v_k\}\subseteq V$ be linearly independent. Prove that if $u\in V$, but $u\notin \Span S$, then $\{u\}\cup S$ is linearly independent.
    \item Prove that there are $u_1, u_2, \dots,u_{n-k}$ such that $v_1, v_2, \dots, v_k, u_1, u_2,\dots, u_{n-k}$ is a basis of $V$.
  \end{enumerate}
\end{prob}

\begin{prob}
  Assume that you have a basis $e_1, e_2,\dots,e_n$ of a finite dimensional vector space $V$ over a field $\mathbb F$. Therefore every vector $v$ can be written as a sum
  $v=v_1e_1+v_2e_2+\dots+v_ne_n$ for some $v_i\in \mathbb F$. Prove that these numbers are unique, that is if $v=v_1e_1+v_2e_2+\dots+v_ne_n=v'_1e_1+v'_2e_2+\dots+v'_ne_n$, then
  $v_i=v'_i$ for all $i$. Hint: $0=v-v$ and $e_i$ are linearly independent.
\end{prob}

\subsection{Subspaces, direct sum and quotient spaces}
As in the case of topological spaces, there are many ways of constructing \textit{new} vector spaces from old. In topology we could construct
new spaces taking a subset of a known topological space, take disjoint unions (sums) of topological spaces, divide topological spaces by some relations
and take product of them. In this subsection we cover first three constructions - fourth one gives a raise to the concept of tensors and multilinear algebra and we will cover
it in great detail later.
Consider a vector space $V$ over field $F$ and a subset $U\subset V$ such that $0\in U$ and for all $v, u\in U, f\in F$, we have $fv+u\in U$. You can check that it is indeed a vector space:
\begin{prob}
  Prove that $fv+u\in U$ for all $v,\, u\in U, f\in F$ is equivalent to: for every $v,\, u\in U$, we have $v+u\in U$ and for every $v\in U, f\in F$ we have $fv\in U$.
\end{prob}

Such a $U$ we call a \textbf{vector subspace} of $V$.

\begin{prob}
  Let $V$ be a finite dimensional vector space and $U\subseteq V$ be a vector subspace. Prove that $U$ is finite dimensional and $\dim U\le \dim V.$
\end{prob}

\begin{prob}
  Let $V$ be a finite dimensional vector space and $U\subseteq V$ be a vector subspace. Prove that $\dim U=\dim V$ iff $U=V$.
\end{prob}

There is also a method of constructing direct sums: assume that you have two vector spaces $V, W$ over \textit{the same field} $F$. We define their direct sum as:
$$V\oplus W=V\times W = \{(v,w) : v\in V,\, w\in W\}$$
with addition and multiplication defined entrywise:
$$a(v,w)+(v', w')=(av+v', aw+w').$$

We often identify $v\in V$ with $(v,0)\in V\oplus W$ and $w\in W$ with $(0,w)\in V\oplus W$. Then $av+bu, a,b\in F, v\in V, u\in U$ should be understood as $(av, bu)\in V\oplus U$.

\begin{prob}
  Prove that each $w\in V\oplus U$ has a \textit{unique} decomposition: $w=v+u,\, v\in V,\,u\in U$. That is if $w=v+u=v'+u'$, then $v=v'$ and $u=u'$ for $v,v'\in V,\, u,u'\in U$.
\end{prob}

\begin{prob}
  Let $U$ and $V$ be finite dimensional vector spaces. Prove that $\dim U\oplus V = \dim U + \dim V.$
\end{prob}

\begin{prob}
  Let $V_1, \,V_2,\, V_n$ be finite dimensional vector spaces. Prove that $$\dim V_1\oplus V_2\oplus\dots\oplus V_n=\dim V_1 + \dim V_2 + \dots + \dim V_n.$$
\end{prob}

Our general definition has a very nice interpretation when we go to subspaces -
now assume that you have two subspaces of $V$: $U,W\subseteq V$ such that $U\cap W=0$. Their \textbf{direct product} is a set:
$$U\oplus W =\{u+v : u\in U,\, v\in V\}$$

\begin{prob}
  Prove that the direct product of two vector subspaces is a special case of the general definition if we identify $U\ni u\leftrightarrow(u,0)\in U\times V$, $V\ni v\leftrightarrow (0,v)\in U\times V$ employed.
\end{prob}

\begin{prob}
  Prove directly that direct product of two vector subspaces of $V$ is a vector subspace of $V$. Hint: check if 0 is inside and use the handy, one-line criterion.
\end{prob}

\begin{prob}
  Let $U$ be a subspace of $V$ and $V$ be a subspace of $W$. Prove that $U$ is subspace of $W$.
\end{prob}

\begin{prob}
  Let $V=\mathbb R^2$ and $U=\{(0, r) : r\in \mathbb R\},~W=\{(r, 0) : r\in \mathbb R\}$. Prove that:
  \begin{enumerate}
    \item $V=U\oplus W$.
    \item Prove that $U\cup V$ is \textit{not} a vector space.
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $U,W\subseteq V$ be two vector subspaces of a finite vector space $V$. Prove that:
  \begin{enumerate}
    \item $U\cap W$ is a subspace of $U$, $W$ and $V$.
    \item $U+W:=\{u+w : u\in U, w\in W\}$ is a vector subspace\footnote{if $U\cap W=\emptyset$ we have just $U+W=U\oplus W$} of $V$
    \item Take a basis $B_i$ of $U\cap W$ and extend it using some vectors $B_U\subseteq U$ such that $B_i\cup B_U$ is a basis of $U$.
      Repeat this procedure of $W$ defining $B_W$ and prove that $V=\Span B_i\cup B_U\cup B_W.$
    \item Prove that $B_i\cup B_U\cup B_V$ is linearly independent. Hint: write the condition of linear independence. Express the linear combination of the elements of $B_U$ as a linear combination of $B_i$ and $B_V$. Why is this linear combination in $U\cap V$? What you can conclude from the fact that $B_i\cup B_U$ is a basis?
    \item Prove that $\dim U+V=\dim U+\dim V - \dim U\cap V$.
  \end{enumerate}
\end{prob}

\subsection{Quotient spaces}
Let us introduce a relation:
\begin{prob}
  Consider vector space $V$ and it's subspace $U$. We introduce a relation on $V$: $v\approx u$ iff $v-u\in V$. Prove that $\approx$ is an equivalence relation.
\end{prob}
We have an equivalence relation, so it splits $V$ into equivalence classes $V/\approx$.

\begin{prob}
  Prove that addition and scalar multiplication on $V/\approx$ are well-defined (independent on the class representative), that is:
  \begin{enumerate}
    \item if $v\approx v'$ and $u\approx u'$, then $v+u\approx v'+u'$
    \item if $\alpha$ is a scalar and $v\approx v'$ are vectors in $V$, then $\alpha v\approx \alpha v'$.
  \end{enumerate}
\end{prob}

\begin{prob}
  Prove that under relation $\approx$, $U$ is identified with 0.
\end{prob}

We have vector addition and scalar multiplication, we have a neutral element - we have a new vector space! This vector space is called \textbf{quotient space} and usually written as
$V/U$.

\begin{prob}
  Let $U\subseteq V$ be finite-dimensional vector spaces. Prove that $\dim V/U=\dim V - \dim U$. Hint: guess what is the basis of $V/U$ starting with basis of $U$ and completing
  it to the basis ov $V$.
\end{prob}

\section{Linear maps}
As continous functions are in some kind, natural mappings between topological spaces, we can define such natural mappings between vector spaces. Let $V$ and $W$ be vector spaces over
the same field $\mathbb F$. We say that a function $L:V\to W$ is \textbf{linear} iff $L(\alpha v + \beta u)=\alpha L(v) + \beta L(u)$ for every $v,u\in V,\, \alpha,\beta\in \mathbb F$.

\begin{prob}
  Let $L:V\to W$ be a function between vector spaces over field $\mathbb F$. Prove that the following sentences are equivalent:
  \begin{enumerate}
    \item $L$ is linear
    \item for every $u,v\in V$ and $\alpha\in \mathbb F$, we have $L(\alpha u+v)=\alpha L(u)+L(v)$
    \item for every $v,u\in V$ we have $L(v+u)=L(v)+L(u)$ and for every $v\in V, \alpha\in \mathbb F$ we have $L(\alpha v) = \alpha L(v)$
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $U$ be a vector subspace of $V$. Prove that \textbf{the inclusion map} $\iota : U\to V$ given as $\iota(u)=u$ is linear.
\end{prob}

\begin{prob}
  Let $U$ be a vector subspace of $V$. Prove that \textbf{the quotient map} $q: V\to V/U$ given as $q(v)=[v]$ is linear.
\end{prob}

\subsection{Kernel and cokernel}
\begin{prob}
  Let $L:V\to W$ be a linear map between vector spaces.
  \begin{enumerate}
    \item Prove that $L(0_V)=0_W$, where $0_V$ is the neutral element in $V$ and $0_W$ is the neutral element in $W$.
    \item Prove that the \textbf{kernel of} $L$ defined as: $\Ker L = \{v\in V : L(v) = 0_W\}$ is a vector subspace of $V$.
    \item Prove that the image of $L$ is a vector subspace of $W$. The dimension of $\Image L$ is called \textbf{the rank} of $L$: $\Rank L = \dim \Image L$.
    \item Let $V=\Span S$. Prove that $\Image L=\Span L(S)$. Here $L(S)$ has meaning $\{L(s) : s\in S\}$.
    \item Prove that if $V$ is finite dimensional, then $\Image L$ is also finite dimensional.
  \end{enumerate}
\end{prob}

Using kernel we can say whether a linear function is injective:
\begin{prob}
  Prove that $\varphi : V\to U$ is injective iff $\Ker \varphi = \{0\}$.
\end{prob}

There is a similar concept, checking the surjectivity. We say that the cokernel of a linear map $L: V\to U$ is a \textit{quotient} vector space: $U/\Image L.$

\begin{prob}
  Prove that $\varphi : V\to U$ is surjective iff $\Coker \varphi$ has exactly one element (is a trivial vector space).
\end{prob}

\subsection{Rank-nullity theorem}
You can prove the rank-nullity theorem:
\begin{prob}
  Prove the \textbf{rank-nullity theorem} - if $V$ is a finite dimensional vector space and $L:V\to W$ is linear, then $\dim \Ker L + \Rank L=\dim V$.
\end{prob}

\begin{prob}
  You can do a beuatiful and simple proof of $\dim U+V = \dim U + \dim V - \dim U\cap V$, where $U$ and $V$ are finite-dimensional subspaces of some greater space $S$.
  Consider a map $L: U\times V\to S$ defined as $L(u,w)=u-w$. Prove that it is a linear map (we give $U\times V$ a linear space structure using entry-wise operations, as in
  $U\oplus V$). What is it's range and kernel?
\end{prob}

\begin{prob}
  Let $\varphi : V\to U$ be a map between finite-dimensional vector spaces. Prove that the \textbf{index} of $\varphi$ defined as $\text{index\,} \varphi=\dim \Ker \varphi - \dim \Coker \varphi$ is equal to:
   $\text{index\,} \varphi=\dim V - \dim U$.\footnote{This is a very important result - index, defined with the help of a chosen function is doesn't in fact depend on this function! A beautiful generalisation of this result, is called Atiyah-Singer index theorem.}
\end{prob}

In the category of topological spaces, homeomorphic spaces have the same topological properties (as connectedness or Haussdorf property). We say that a map $L:W\to W$ is a
\textbf{isomorphism\footnote{from the Ancient Greek \textit{isos} - equal and \textit{morphe} - shape} of vector spaces} iff is bijective and both $L$ and $L^{-1}$ are linear and
we can say that abstract\footnote{Later we will define additional structures on vector spaces for which just arbitrary isomorphisms are not sufficient} vector spaces are identical if they are isomorphic.

\begin{prob}
  Prove that $V$ and $\{0\}\times V$ are isomorphic. (Do you remember the direct sum of two subspaces? In fact we used there this isomorphism).
\end{prob}

You can also classify all finite-dimensional vector spaces over the same field, up to isomorphism:

\begin{prob}
  Prove that two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension. Hint: right implication - rank-nullity theorem, left - write down bases.
\end{prob}

As we remember $\F^n$ is an $n$-dimensional vector space over $\F$. Therefore many people learn how to work only on them, as all the vector space properties can be
just transferred from $\F^n$. We will not do it, as the isomorphism is usually not-unique and does not preserve additional structures, very important in more advanced geometry.
Such chosen isomorphisms will later give a raise to objects called metric tensors and symplectic forms.

\subsection{Exact sequences}
In algebraic topology topological spaces are investigated by assigning to them algebraic objects, like vector spaces or groups. One of frequently-occuring concept are exact sequences.
Consider a sequence of vector spaces\footnote{Or, more generally, Abelian groups as we will need only to use such properties as kernel, image and quotient spaces.} $V_i,~i\in \{1,2,\dots\}$ and linear maps between them $\varphi_i : V_i\to V_{i+1}$. It is often written as:
$$\dots\stackrel{\varphi_{i-1}}{\longrightarrow} V_i\stackrel{\varphi_i}\longrightarrow V_{i+1} \stackrel{\varphi_{i+1}}\longrightarrow \dots,$$
and map $\varphi_i$ is referenced as $V_i\to V_{i+1}$. We say that the sequence is \textbf{exact} if $\Image \varphi_{i}=\Ker \varphi_{i+1}$ for all $i$.

\begin{prob}
  Prove that if sequence of vector spaces $V_i$ and maps $\varphi_i : V_i\to V_{i+1}$, is exact, then $\varphi_i\circ \varphi_{i+1}=\textbf{0}$, where \textbf{0} is a null \footnote{Later we will refer to this map just as 0 - now this symbol has at least three meanings! It can be an additive neutral element of a field, a neutral element in a vector space or a linear map!} map $\textbf{0}: V_i\to V_{i+2}$ defined as $\textbf{0}(v)=0$.
\end{prob}

Consider a $0$-dimensional vector space $\{0\}$, which is usually abbreviated to just 0\footnote{Similar notational discrepancy was in the set theory - we wanted to write
$f^{-1}(a)$ for $f^{-1}(\{a\})$.

}. An exact sequence (Remember! Here 0 means $\{0\}$):
$$0\stackrel{\varphi_1}{\longrightarrow} V_2\stackrel{\varphi_2}{\longrightarrow} V_3\stackrel{\varphi_3}{\longrightarrow} V_4\stackrel{\varphi_4}{\longrightarrow} 0$$
is called a \textbf{short exact sequence}. Longer exact sequences are called, obviously, \textbf{long exact sequences}.

\begin{prob}
  Prove the following:
  \begin{enumerate}
    \item sequence $V\stackrel{\varphi}{\longrightarrow}U\longrightarrow 0$ is exact iff $\varphi$ is surjective. Hint: what is kernel of the map $U\to 0$?
    \item sequence $0\longrightarrow V\stackrel{\varphi}{\longrightarrow}U$ is exact iff $\varphi$ is injective. Hint: do you remember how injectivity is related to some kernel?
    \item sequence $0\longrightarrow V\stackrel{\varphi}{\longrightarrow}U\longrightarrow 0$ is exact iff $V$ and $U$ are isomorphic.
    \item short sequence
      $0\longrightarrow V_2\stackrel{\varphi_2}{\longrightarrow} V_3\stackrel{\varphi_3}{\longrightarrow} V_4\longrightarrow 0$
      is exact iff $\varphi_2$ is injective and $\varphi_3$ is surjective.
  \end{enumerate}
\end{prob}

\begin{prob}
  Use the rank-nullity theorem and prove that if $0\to V_0\to V_1\to\dots\to V_{n-1}\to V_n\to 0$ is exact, then
  $$0=\sum_{i=0}^n (-1)^i \dim V_i.$$
\end{prob}

\begin{prob}
  Consider a linear map $L : V\to U$. Prove that the sequence:
  $$0\longrightarrow \Ker L\stackrel{\kappa}{\longrightarrow} V\stackrel{L}{\longrightarrow} U\longrightarrow \Coker L \longrightarrow 0$$
  is exact, where $\kappa : \ker L \to V$ is inclusion map: $\kappa :\Ker L\ni l\to l\in V$.
\end{prob}

\begin{prob}
  Let $L : V\to U$ be a map between finite-dimensional vector spaces. Prove once again that the $\text{index\,} L:=\dim \Ker L - \dim \Coker L =\dim V - \dim U$.
\end{prob}

\section{Dual spaces}
Consider a vector space $V$ over a field $\F$ and the set of all \textit{linear} functions from $V$ to $\F$. This set is called
\textbf{the dual space} $V^*$. Apparently, this set is a vector space!
\begin{prob}
  Prove that $V^*$ is a vector space by:
  \begin{enumerate}
    \item Finding the neutral element. Hint: function that always yields 0 is linear.
    \item Proving that addition and scalar multiplication can be defined, so if $\mu,\,\omega \in V^*$ and $a\in \F$, then function $a\mu +\omega$, defined as:
     $(a\mu +\omega)(v) = a\cdot \mu(v) +\omega(v)$ for all $v\in V$, is linear.
  \end{enumerate}
\end{prob}

Sometimes we write $\BL \omega, v\BR := \omega(v)\in \F$ for $v\in V$ and $\omega \in V^*$.

\begin{prob}
  Let $V^{**}=(V^*)^*$ be the dual space of the dual space to $V$. We will try to prove that, in some sense, $V$ is a subset of it. Prove that:
  \begin{enumerate}
    \item For each $v\in V$ there is a $\tilde v\in V^{**}$ such that for every $\omega \in V^*$ such that $\BL \tilde v, \omega\BR=\BL \omega, v\BR$
    \item Prove that the map $v\mapsto \tilde v$ is a monomorphism (an injective linear map).
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $f : V\to U$ be a linear map between finite dimensional spaces. We define $f^*:U^*\to V^*$ as follows:
  \begin{figure}
    \centering
    \begin{tikzcd}
      V \arrow[rd, "f^*(\omega)"] \arrow[rr, "f"] &       & U \arrow[ld, "\omega"]\\
                                             &  \R   &
    \end{tikzcd}
  \end{figure}
  Prove that such map is linear and that $f^*\circ \omega = \omega\circ f$ for every $\omega\in U^*$. Here we see how a function sending vectors in one direction can naturally
  induce a linear map that ,,pulls-back" dual vectors.
\end{prob}

Therefore we can think about $V$ as a set of \textit{some} linear functions on $V^*$ and write $v(\omega)$ as well as $\omega(v)$. The word \textit{some} is necessary -
we have proven that it is a monomorphism, but we don't know whether it is an isomorphism\footnote{Unfortunately for vector spaces $V$ of \textit{infinite} dimension, $V^*$ is greater than $V$ and consequently $V^{**}$ is greater than both $V^*$ and $V$.}! Fortunately, there is a nice theorem for vector spaces of finite dimension:

\begin{prob}
  Let $V$ be a finite dimensional space with basis $e_1,e_2,\dots,e_n$. Prove that $\mu^1, \mu^2,\dots,\mu^n\in V^*$ defined as
    $$\mu^i(e_j)=\begin{cases}1 \text{ if } i=j\\ 0 \text{ otherwise}\end{cases},$$
  is a basis of $V^*$.
\end{prob}

As we know that in this case $V$ and $V^*$ have the same dimension, we can find an isomorphism between them - so they can be identified.
Unfortunately, for our future needs, just an isomorphism is not enough. We will be interested mostly in \textbf{natural} or \textbf{canonical} isomorphisms, that
is isomorphisms that don't need a basis to be defined. For example, definitions of quotient spaces or $V\times \{0\}\approx V$ did not invoke bases, so these are examples of naturally
isomorphic spaces. It has a great geometrical meaning - we don't want to choose one special basis, it would be rather more convenient to use \textit{any} basis we would like to.
This is exactly what we do in differential geometry - we manipulate with objects that are basis and coordinate independent, what allows us to prove theorems in bases that fit to
the problem.

\section{Tensors}
Consider vector spaces $V_1, V_2,\dots V_n$ and a function $f: V_1\times V_2\times\dots\times V_n\to \F$. We say that $f$ is \textbf{multilinear} if for every $i$ we have:
$$f(v_1,\dots, v_i+\alpha v_i', \dots, v_n) = f(v_1,\dots v_i, \dots, v_n) + \alpha f(v_1,\dots, v_i', \dots, v_n).$$

\begin{prob}
  Consider a multilinear function $f$ from $V_1\times V_2\times \dots \times V_n$ to $\F$. Prove that $f$ is a linear mapping from $V_1\oplus V_2\oplus\dots\oplus V_n$,
  to $\F$.
\end{prob}

But being linear and multilinear is not equivalent:

\begin{prob}
  Find such $f$ that $f$ is a linear mapping from $V_1\oplus V_2\oplus\dots\oplus V_n$,to $\F$, but $f$ \textit{cannot} be treated as a mutlilinear function from
  $V_1\times V_2\times \dots \times V_n$ to $\F$.
\end{prob}

\subsection{Universal property}

We should on some way improve this vulnerability by creating an object that would interplay between linear and multilinear mappings.
Consider finite dimensional $V$ and $W$. We want to create a vector space called \textbf{tensor produt} and denoted as $V\otimes W$ such that:
\begin{itemize}
  \item for $(v,w)\in V\times W$, there is a vector $v\otimes w\in V\otimes W$
  \item map $\varphi:V\times W\to V\oplus W$ is bilinear, where $\varphi(v,w)=v\otimes w$. That is we require: $(v+\alpha v')\otimes w = v\otimes w + \alpha v'\otimes w$
    and $v\otimes (w+\alpha w') = v\otimes w + \alpha v\otimes w'$.
\end{itemize}
Then such vector space $V\otimes W$ and map $\varphi$ have \textbf{universal factorisaction property}, that is let $f : V\times W\to U$ be any bilinear map. Then there is \textit{unique} linear function $\tilde f:V\otimes W\to U$ such that $f = \tilde f\circ\varphi$. It can be expressed in terms of the following diagram:

\begin{figure}
  \centering
  \begin{tikzcd}
    V\times W \arrow[rd, "f"] \arrow[r, "\varphi"] & V\otimes W \arrow[d, "\tilde f"]\\
    & U
  \end{tikzcd}
\end{figure}

Using this property, we can prove that tensor product of vector spaces is unique, commutative and associative.

\begin{prob}
  Let $V$ and $W$ be vector spaces. Assume that there is a space $T$ and bilinear map $\varphi$ have the universal property and space $U$ an bilinear map $\theta$ also have
  the universal property. Prove that $T$ and $U$ are naturally isomorphic.
\end{prob}

\begin{prob}
  Define $f:V\times W\to W\otimes V$ as $f(v,w)=w\otimes v$. Using the universal property, prove that there is an isomorphism $v\otimes w\mapsto w \otimes v$.
\end{prob}

\begin{prob}
  Prove that there is a canonical isomorphism between $(U\otimes V)\otimes W$ and $U\otimes (V\otimes W)$ by:
  \begin{enumerate}
    \item Proving that map $\lambda_u : V\times W\to (U\otimes V)\otimes W$ given by $\lambda_u(v,w)=(u\otimes v)\otimes w$ is bilinear.
    \item As $\lambda_u$ is bilinear, you can find linear map $\tilde \lambda_u: V\otimes W\to (U\otimes V)\otimes W$. Prove that map
      $\Lambda : U\times (V\otimes W)\to (U\otimes V)\otimes W$ given by $\Lambda (u, \omega)=\tilde\lambda_u(\omega)$ is bilinear. Find therefore a map
      $U\otimes (V\otimes W)$ to $(U\otimes V)\otimes W$.
  \end{enumerate}
\end{prob}

\subsection{Construction}
But a question arises - is it possible to contruct such space? We give two equivalent constructions. Consider a set $S$ and a field $\F$.
\begin{prob}
  Let $V$ be a set of all functions from set $S$ to field $\F$ such that $f(s)=0$ for all but finitely many $s\in S$. Prove that $V$ is a vector space.
\end{prob}

We usually write $f\in V$ as $f=a_1s_1 + a_2s_2+\dots+a_ns_n$, where $s_i\in S$ are some elements and $a_i=f(s_i)\in F$.

\begin{prob}
  Consider a set $S$ and a free vector space generated by it $V_S$. We define inclusion mapping $\iota : S\to V_S$ as $S\ni s \mapsto s\in V_s$. Prove that for every function
  $f: S\to U$, where $U$ is a vector space, there is a unique linear map $\tilde f: V_S\to U$ such that $f=f\circ \iota$. This can be written as a commutative diagram:

  \begin{figure}
    \centering
    \begin{tikzcd}
      S\arrow[rd, "f"] \arrow[r, "\iota"] & V_S \arrow[d, "\tilde f"]\\
      & U
    \end{tikzcd}
  \end{figure}
\end{prob}

\begin{prob}
  Let $V$ and $W$ be vector spaces over a field $\F$. Let $A$ be a free vector space over $V\times W$.
  Consider set $S$ containing elements of the forms
  $$(v+v',w+w')-(v', w'),~~(\alpha v, \alpha w)-\alpha (v,w)$$
  for $(v,w) \in V\times W$ and it's free vector space $B$.
  Prove that $V\oplus U$ is naturally isomorphic to $A/B$.
\end{prob}

\begin{prob}
  Let $V$ and $W$ be vector spaces over a field $\F$. Let $A$ be a free vector space over $V\times W$. Consider a set $S$ containing all the elements of the forms:
  \begin{align}
    (v+v',w)-(v,w)-(v',w),~(\alpha v, w)-\alpha(v,w)\\
    (v,w+w')-(v,w)-(v,w'),~(v, \alpha w)-\alpha(v,w)
  \end{align}
  and it's free vector space $B$. Prove that $A/B$ has the universal property of tensor product, with $v\otimes w = [(v,w)]$.
\end{prob}

For finite dimensional spaces another construction is possible (as we know - that must be naturally isomorphic to the previous one).

\begin{prob}
  Let $V$ and $W$ be finite dimensional over $\F$.
  Consider a set $S$ of all bilinear functions from $V^*\times W^*\to \F$. As we remember, for finite dimensional $V$ we have a natural isomorphism $V\approx V^{**}$,
  so we can write $v(\nu)\in \F$ for $v\in V,\,\nu\in V^*$. Let's define: $v\otimes w(\nu, \omega) = v(\nu)\times w(\omega)$. Prove that:
  \begin{enumerate}
    \item $v\otimes w\in S$ (so it must be a bilinear function)
    \item $\{v_i\otimes w_j\}$ form a basis of $S$ if $\{v_i\}$ and $\{w_j\}$ are bases of $V$ and $W$.
    \item $V\otimes W$ defined as above is naturally isomorphic to $S$.
    \item $\dim V\otimes W = \dim V \cdot \dim W$
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $V, W, U$ be finite dimensional vector spaces over the same field. Prove that there is a natural isomorphism:
  $$V\otimes(W\oplus U)\approx (V\otimes W) \oplus (V\otimes U).$$
\end{prob}

\begin{prob}
  Prove that for finite dimensional $V$ and $W$, there is a natural isomorphism between $\Hom(V,W)$ and $V^*\otimes W$.
\end{prob}

\begin{prob}
  Prove that for finite dimensional $V$, $\End V$ is naturally isomorphic to to $\End V^*$.
\end{prob}

% \subsection{Exterior algebra}
